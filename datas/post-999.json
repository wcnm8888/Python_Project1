{
    "link": "https://bbs.nga.cn/read.php?tid=40403636",
    "title": "[求助] CUDA中使用C++类，如何从内存访问视角解析device端的代码调用和变量访问",
    "post_time": "2024-06-03 03:47",
    "uid": "19123105",
    "content": "起初是想做一个GPU上的环形指针缓冲区，以前在CPU上是用C++做成类的，发现要转化到GPU上有很多不是很懂的地方。逻辑上的代码大概是这样(按照host端的逻辑先弄了个解释性的代码，简化起见所有成员函数和变量都是public)",
    "replies": [
        {
            "mid": "3260753",
            "mtime": "2024-06-03 10:11",
            "mcontent": "你都会自问自答了 cudaRing1D不能再host端创建在device上用，理论上你可以在host上创建 然后deepcopy到device上，正常来讲不要这么做， 环形队列不就是array + index的事情吗 ，这里的场景应该是host端从环形队列里面取出buffer给cuda kernel用不要把简单的事情弄得复杂 把面向对象那套生搬到cuda上 cuda编程范式一般都是过程式的 cuda kernel应该只管算，逻辑越少越好"
        },
        {
            "mid": "19123105",
            "mtime": "2024-06-03 11:01",
            "mcontent": "我这个的应用场景在数据获取，会不断的取数，交给GPU算。GPU里面还预计会有好几个环节，前一个环节算出来的数据按照新的数据结构交给后一个环节的device函数来算。按照之前的CPU编程想法，每个环节之间都会有个环形缓冲区。但我不清楚或者比较生疏的是，在device端的SM单元里，是不是也相当于是一个本地的CPU那样的环境，可以将其视作有自己的完善的代码运作逻辑，比如有中断，有线程管理，通过上下文环境可以和CPU端那样调度本地的各种代码段和变量。然后在device端创建的类运行代码的逻辑就和CPU端一样(除了不能直接访问另一侧的地址内容)。如果是，或者大部分是只有少量限制，那我就偏向于在device端创建一个cudaRing1D这样的类，至少device的两个工序之间生产消费数据就走这个ring了，不然两个device的线程组之间，如何交换数据感觉不好弄。做在同一个流里，程序结构和逻辑分段不分开，不好。不做在同一个流里，简单的每次前一个环节生产数据，传回CPU端，再由下一个环节的流传回GPU操作，冗余浪费了。"
        },
        {
            "mid": "3260753",
            "mtime": "2024-06-03 11:45",
            "mcontent": "我觉得你还是先把基础概念入门再写代码 把官方guideline读一遍 应该都不会有类似的问题 你这些问题都是因为你不熟悉cuda编程范式然后又自己脑补一堆导致的我理解你这里应该是host端按顺序launch kernel就行的事情，两个kernel之间直接通过显存共享数据就好，前一个kernel写到显存里的数据又不会消失，为什么还会在host/device之间传，你只需要传回你host端最终需要的东西就行"
        },
        {
            "mid": "42252406",
            "mtime": "2024-06-03 14:39",
            "mcontent": "不明白为啥要来回传，还要在显存弄ring。。。这种业务应该是内存映射解决的事情。显存就当白板。显存的数据结构放在内存里面。"
        },
        {
            "mid": "19123105",
            "mtime": "2024-06-03 15:14",
            "mcontent": "我大概明白您的说意思了，就是把GPU上的工作只放到worker::gpuRun()线程那一个层次，里面没有while循环，只是一个一次性执行调度的线程，调度结束后kernel结束。全部的处理环节封装还是在CPU端，主类来做while循环不断派发worker::gpuRun()线程。派发worker::gpuRun()线程的时候由主类自己先把ring的某个index指针拿到，作为参数给worker::gpuRun()来访问。但是随之而来还有个问题(不好意思，确实没学懂cuda的很多地方，只好硬着头皮请教一下)，按上面的方法，我大概会需要在worker::run()这个函数中不断的循环提交kernel，这一批次提交完，然后看看分配的ring里面哪些节点准备好了写入新数据完毕，下一批次又拿这批写好内容的index指针做参数，提交新的kernel。这样会不断的创建kernel或者异步stream来向GPU派发device函数的调度。这些stream会不会类似线程那样，有重复创建和回收的效率衰减问题。cuda对stream的利用有没有类似线程池那样的，预先创建好一堆stream然后重复用不同参数调用的模式。"
        },
        {
            "mid": "3260753",
            "mtime": "2024-06-03 16:52",
            "mcontent": "你把stream理解成线程池就行了 向同一个stream提交的kernel会顺序完成 不同stream则会并行 本来用法就是预先创建stream 但是这个创建目的不是为了线程重复创建和回收的问题，而是为了你的kernel用不满SM时候的并发。。对cuda来说根本没cpu/系统线程这回事，但是kernel/cuda api的调用始终是有overhead，不管你有多少stream你这里的模式应该是host端按顺序调kernel直到完成你的最终目的，为什么要把不同阶段的kernel混在一个while loop里面，一个函数run kernel-> wait kernel -> ... 走到黑不就完了，在这之上再设计你的host code如果需要一个task的不同phase kernel之间的并行，自己去看cuda graph"
        }
    ]
}