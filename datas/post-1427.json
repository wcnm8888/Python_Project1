{
    "link": "https://bbs.nga.cn/read.php?tid=39494132",
    "title": "大模型微调咨询",
    "post_time": "2024-03-06 14:52",
    "uid": "60881205",
    "content": "我部署了qwen-7b的模型，并使用LoRA方法对模型进行了微调，微调后模型对于新知识的表现很像传统的key- value数据库。好像并没有表现出智能。以上操作基本上照猫画虎。",
    "replies": [
        {
            "mid": "5041763",
            "mtime": "2024-03-06 15:12",
            "mcontent": ""
        },
        {
            "mid": "919214",
            "mtime": "2024-03-06 19:39",
            "mcontent": "用lora来训练知识在小模型上很容久把模型训傻(过拟合)我们的应对方案：1. 训练数据中增加大量正常对话的数据集2. 使用rag来注入知识3. 减少 epoch微调这事儿坑挺多的"
        }
    ]
}