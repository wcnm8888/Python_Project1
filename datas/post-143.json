{
    "link": "https://bbs.nga.cn/read.php?tid=43378529",
    "title": "老哥们为什么我本地部署的deepseek模型很多回答都是在胡编乱造呢",
    "post_time": "2025-02-26 01:53",
    "uid": "38828889",
    "content": "模型用的是这两个",
    "replies": [
        {
            "mid": "24060861",
            "mtime": "2025-02-26 06:17",
            "mcontent": "因为这只是蒸馏模型，不是deepseek本身，而且参数量过少DeepSeek-R1-Distill-Llama-8B 这里面的每个参数都是什么意思模型名称 DeepSeek-R1-Distill-Llama-8B 的每个部分通常代表以下含义：1. DeepSeek含义：模型开发者或机构的名称，这里是中国的深度求索(DeepSeek)公司。背景：DeepSeek 专注于人工智能领域，尤其在大语言模型(LLM)和生成式 AI 技术方面有较多成果。2. R1含义：Release 1(第 1 版)的缩写，表示模型的版本号。作用：用于区分同一系列模型的不同迭代版本。例如，R1 可能是初始版本，后续可能会有 R2、R3 等改进版。3. Distill含义：指模型通过 知识蒸馏(Knowledge Distillation) 技术训练得到。技术原理：知识蒸馏是一种模型压缩方法，通过让较小的模型(学生模型)模仿更大、更复杂的模型(教师模型)的输出或中间特征，从而在小模型中保留大模型的能力。优势：减少模型参数量和计算资源需求，同时保持较高的性能。适合部署在资源受限的场景(如移动端、边缘设备)。4. Llama含义：模型架构基于 Meta 的 LLaMA 系列模型(如 LLaMA-1/2)。背景：LLaMA 是 Meta 开源的大语言模型架构，以其高效性和高性能著称。许多后续模型(如 Alpaca、Vicuna)基于 LLaMA 进行微调或改进。特点：使用 Transformer 架构。支持长上下文理解和生成任务。5. 8B含义：模型的参数量为 80 亿(8 Billion)。重要性：参数量直接影响模型的能力和资源消耗。8B 参数的模型属于中等规模，适合在消费级 GPU(如 RTX 3090/4090)上运行。对比参考：GPT-3：175B 参数LLaMA-2：7B/13B/70B 参数Mistral-7B：73 亿参数总结：模型特点定位：轻量级、高效的大语言模型，适合资源有限场景。技术路径：基于 LLaMA 架构，通过知识蒸馏压缩模型规模。适用场景：文本生成、问答、对话系统等任务，尤其适合需要快速响应或本地化部署的应用。如果需要进一步了解技术细节或应用方法，可以查阅 DeepSeek 的官方文档或论文。"
        },
        {
            "mid": "544943",
            "mtime": "2025-02-26 07:35",
            "mcontent": "首先这是用ds的回答训练的其他架构的模型，其次量化到int8了，然后模型规模还很袖珍，自然容易胡说八道"
        },
        {
            "mid": "42685479",
            "mtime": "2025-02-26 11:17",
            "mcontent": "你想搞本地部署最少也需要7900xtx，我负责任的说24G以下的显存真的别学别人部署local LLM。部署一堆1.5B或者7B int4的有什么意义呢…"
        },
        {
            "mid": "38828889",
            "mtime": "2025-02-26 11:31",
            "mcontent": "刚接触不了解嘛"
        },
        {
            "mid": "61861923",
            "mtime": "2025-02-26 22:29",
            "mcontent": "32B以下玩玩得了，不如用网页或者api。而且deepseek小参数模型都是蒸馏的千问llama这些，为啥不用原版？最近阿里新出的qwq非常好用，chat.qianwen.ai，比国内网页版和app的通义强多了。"
        }
    ]
}