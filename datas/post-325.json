{
    "link": "https://bbs.nga.cn/read.php?tid=42596072",
    "title": "请问macbook本地部署大模型怎么选模型",
    "post_time": "2024-11-29 15:55",
    "uid": "37101367",
    "content": "一直都是用的chatgpt，豆包，智谱，kimi这种大模型，现在想自己在本地部署一个玩玩，这块不太懂，不知道怎么选模型",
    "replies": [
        {
            "mid": "37101367",
            "mtime": "2024-11-30 16:15",
            "mcontent": "自己捞一下"
        },
        {
            "mid": "64703914",
            "mtime": "2024-12-01 00:08",
            "mcontent": "你搜一下几个流行的开源大模型，例如 Facebook 的 Llama 3，国产的 qwen 啥的，都有详细本地部署教程。还有一个叫 ollama 的开源工具，可以让你部署多种不同的开源大模型，已经做得非常傻瓜化了。"
        },
        {
            "mid": "37101367",
            "mtime": "2024-12-01 10:17",
            "mcontent": "非常感谢，之前了解到有个模型非常适合苹果电脑，不知道是哪一个呢"
        },
        {
            "mid": "64703914",
            "mtime": "2024-12-01 14:11",
            "mcontent": "很多开源大模型都同时提供了不同参数数量的版本供人选择，比如有 2B 版(表示20亿参数)，7B版(表示70亿参数)，参数越多，模型越聪明，相应地对电脑的算力和显存及内存的要求就越高，你可以先尝试跑小一点的模型，然后再慢慢尝试换成参数更大的模型"
        },
        {
            "mid": "65633902",
            "mtime": "2024-12-01 16:18",
            "mcontent": "笔记本想部署大模型有点难度吧，一般人想玩大模型最应该注意的硬件参数是显存，mac办公本的定位恐怕不太支持"
        },
        {
            "mid": "37101367",
            "mtime": "2024-12-01 18:09",
            "mcontent": "好的，非常感谢，我慢慢尝试一下"
        },
        {
            "mid": "37101367",
            "mtime": "2024-12-01 18:11",
            "mcontent": "之前看别人说过我这笔记本应该没问题，试着玩玩，不行就弄小一点的"
        },
        {
            "mid": "64703914",
            "mtime": "2024-12-01 18:19",
            "mcontent": "他这笔记本的硬件配置比大多数台式机电脑的配置都要强。另外，Google Germa 发布时，最小的模型号称连手机都能跑得动。试试肯定是没问题的，效果如何就要实测了，如果跑起来了，想要找更强算力的机器，也可以去租各种云服务器，各种配置都有。"
        },
        {
            "mid": "37868766",
            "mtime": "2024-12-01 20:24",
            "mcontent": "部署的话去搜ollama，挺容易的，还有webui，看上去就跟chatgpt一样另外，Mac电脑尽量找带mlx的模型文件，运行起来会稍微快一点最后，就自己这几天玩下来的感受，Mac能跑，但是用起来挺难受的，会让你觉得不如给openai充钱  文字生成的模型推荐千问(qwen)，中文理解得不错，热度也挺高。48g内存的话或许能跑32b规模图像生成的话去找stable diffusion相关的模型，运行工具推荐drawthings，似乎有对Mac的优化，运行效果比常见的webui会好一些，关键是易用。这边的内存需求基本都能满足"
        },
        {
            "mid": "37101367",
            "mtime": "2024-12-02 10:18",
            "mcontent": "谢谢大佬，步骤很详细，我来仔细研究一下"
        },
        {
            "mid": "112704",
            "mtime": "2024-12-05 09:37",
            "mcontent": "ollama后端部署，webui前段，年初玩的时候，qwen比llama好用，根据内存大小选量化等级，总结，就也是玩玩，和gpt这种成熟的服务器集群算力的商业模型，完全不是一个东西。"
        },
        {
            "mid": "37101367",
            "mtime": "2024-12-05 12:34",
            "mcontent": "好的，谢谢大佬提供的另一个方向，我回来研究下"
        },
        {
            "mid": "14316607",
            "mtime": "2024-12-06 14:35",
            "mcontent": "能搞明白自己需要的强化方向肯定是自己玩儿好，不过光一个SFT需要的训练卡资源就卡住大部分人了，别提什么post-pretrain，RL了充钱确实性价比高些"
        },
        {
            "mid": "41133292",
            "mtime": "2024-12-12 21:03",
            "mcontent": "我MacBook Pro 16GB M2，用的ollama，跑qwen 7B 参数是没啥问题的，供参考"
        },
        {
            "mid": "37101367",
            "mtime": "2024-12-13 06:56",
            "mcontent": "非常感谢，可以做个参考了"
        },
        {
            "mid": "66252135",
            "mtime": "2024-12-16 20:45",
            "mcontent": "你这个配置，7B，13B左右的模型应该随便跑，我用argo这个软件，相当于一个本地的coze，把ollama+自己搭建智能体的客户端一起搞定了，macbook上的体验做的还不错，可以自己试不同的模型 + 知识库，[https:://www.xark-argo.com/ 此网页不属于本网站，不保证其安全性  继续访问       取消 https://www.xark-argo.com/ https://www.xark-argo.com/]"
        },
        {
            "mid": "37101367",
            "mtime": "2024-12-16 21:56",
            "mcontent": "最近正在研究，谢谢大佬的参考，存下来"
        }
    ]
}